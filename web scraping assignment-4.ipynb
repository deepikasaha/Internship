{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38597e31",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e46a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9055fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "page_content = response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "table = soup.find(\"table\", class_=\"wikitable sortable jquery-tablesorter\")\n",
    "video_data = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    rank = cells[0].text.strip()\n",
    "    name = cells[1].text.strip()\n",
    "    artist = cells[2].text.strip()\n",
    "    upload_date = cells[3].text.strip()\n",
    "    views = cells[4].text.strip()\n",
    "    video_data.append({\n",
    "        \"Rank\": rank,\n",
    "        \"Name\": name,\n",
    "        \"Artist\": artist,\n",
    "        \"Upload date\": upload_date,\n",
    "        \"Views\": views\n",
    "    })\n",
    "for video in video_data:\n",
    "    print(video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa931365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "fixtures_link = soup.find('a', {'href': '/international/fixtures'})\n",
    "fixtures_url = url + fixtures_link['href']\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.text, 'html.parser')\n",
    "fixtures = fixtures_soup.find_all('div', {'class': 'fixture__format-info'})\n",
    "\n",
    "for fixture in fixtures:\n",
    "    series = fixture.find('span', {'class': 'u-unskewed-text'}).text.strip()\n",
    "    place = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "    date = fixture.find('div', {'class': 'fixture__full-date'}).text.strip()\n",
    "    time = fixture.find('span', {'class': 'fixture__time'}).text.strip()\n",
    "\n",
    "    print(\"A) Series:\", series)\n",
    "    print(\"B) Place:\", place)\n",
    "    print(\"C) Date:\", date)\n",
    "    print(\"D) Time:\", time)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cffa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indian_economy_link = soup.find('a', {'href': '/economy/india-statistics.php'})\n",
    "indian_economy_url = url + indian_economy_link['href']\n",
    "\n",
    "indian_economy_response = requests.get(indian_economy_url)\n",
    "indian_economy_soup = BeautifulSoup(indian_economy_response.text, 'html.parser')\n",
    "\n",
    "\n",
    "state_gdp_link = indian_economy_soup.find('a', {'href': '/economy/indian-states-gdp.php'})\n",
    "state_gdp_url = url + state_gdp_link['href']\n",
    "\n",
    "state_gdp_response = requests.get(state_gdp_url)\n",
    "state_gdp_soup = BeautifulSoup(state_gdp_response.text, 'html.parser')\n",
    "\n",
    "table = state_gdp_soup.find('table', {'id': 'table_id'})\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows[1:]: \n",
    "    columns = row.find_all('td')\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gsdp_1819 = columns[2].text.strip()\n",
    "    gsdp_1920 = columns[3].text.strip()\n",
    "    share_1819 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "\n",
    "    print(\"A) Rank:\", rank)\n",
    "    print(\"B) State:\", state)\n",
    "    print(\"C) GSDP(18-19):\", gsdp_1819)\n",
    "    print(\"D) GSDP(19-20):\", gsdp_1920)\n",
    "    print(\"E) Share(18-19):\", share_1819)\n",
    "    print(\"F) GDP($ billion):\", gdp_billion)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dab799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "trending_repos_link = soup.find('a', {'href': '/trending'})\n",
    "trending_repos_url = \"https://github.com\" + trending_repos_link['href']\n",
    "\n",
    "trending_repos_response = requests.get(trending_repos_url)\n",
    "trending_repos_soup = BeautifulSoup(trending_repos_response.text, 'html.parser')\n",
    "\n",
    "repo_cards = trending_repos_soup.find_all('article', {'class': 'Box-row'})\n",
    "\n",
    "for repo_card in repo_cards:\n",
    "    repo_title = repo_card.find('h1', {'class': 'h3'}).text.strip()\n",
    "    repo_desc = repo_card.find('p', {'class': 'col-9'}).text.strip()\n",
    "    contributors_count = repo_card.find('a', {'class': 'Link--muted'}).text.strip()\n",
    "    language_used = repo_card.find('span', {'itemprop': 'programmingLanguage'})\n",
    "    language_used = language_used.text.strip() if language_used else 'Not specified'\n",
    "\n",
    "    print(\"A) Repository title:\", repo_title)\n",
    "    print(\"B) Repository description:\", repo_desc)\n",
    "    print(\"C) Contributors count:\", contributors_count)\n",
    "    print(\"D) Language used:\", language_used)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "hot_100_link = soup.find('a', {'href': '/charts/hot-100/'})\n",
    "hot_100_url = \"https://www.billboard.com\" + hot_100_link['href']\n",
    "\n",
    "\n",
    "hot_100_response = requests.get(hot_100_url)\n",
    "hot_100_soup = BeautifulSoup(hot_100_response.text, 'html.parser')\n",
    "\n",
    "\n",
    "song_cards = hot_100_soup.find_all('li', {'class': 'chart-list__element'})\n",
    "\n",
    "for song_card in song_cards:\n",
    "    song_name = song_card.find('span', {'class': 'chart-element__information__song'}).text.strip()\n",
    "    artist_name = song_card.find('span', {'class': 'chart-element__information__artist'}).text.strip()\n",
    "    last_week_rank = song_card.find('span', {'class': 'chart-element__meta text--center color--secondary text--last'}).text.strip()\n",
    "    peak_rank = song_card.find('span', {'class': 'chart-element__meta text--center color--secondary text--peak'}).text.strip()\n",
    "    weeks_on_board = song_card.find('span', {'class': 'chart-element__meta text--center color--secondary text--week'}).text.strip()\n",
    "\n",
    "    print(\"A) Song name:\", song_name)\n",
    "    print(\"B) Artist name:\", artist_name)\n",
    "    print(\"C) Last week rank:\", last_week_rank)\n",
    "    print(\"D) Peak rank:\", peak_rank)\n",
    "    print(\"E) Weeks on board:\", weeks_on_board)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table', {'class': 'in-article sortable'})\n",
    "rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all('td')\n",
    "    book_name = columns[1].text.strip()\n",
    "    author_name = columns[2].text.strip()\n",
    "    volumes_sold = columns[3].text.strip()\n",
    "    publisher = columns[4].text.strip()\n",
    "    genre = columns[5].text.strip()\n",
    "\n",
    "    print(\"A) Book name:\", book_name)\n",
    "    print(\"B) Author name:\", author_name)\n",
    "    print(\"C) Volumes sold:\", volumes_sold)\n",
    "    print(\"D) Publisher:\", publisher)\n",
    "    print(\"E) Genre:\", genre)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8faa595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "tv_series_list = soup.find('div', {'class': 'lister-list'})\n",
    "\n",
    "tv_series_items = tv_series_list.find_all('div', {'class': 'lister-item'})\n",
    "\n",
    "for tv_series_item in tv_series_items:\n",
    "    name = tv_series_item.find('h3', {'class': 'lister-item-header'}).find('a').text.strip()\n",
    "    year_span = tv_series_item.find('span', {'class': 'lister-item-year'}).text.strip()\n",
    "    genre = tv_series_item.find('span', {'class': 'genre'}).text.strip()\n",
    "    run_time = tv_series_item.find('span', {'class': 'runtime'}).text.strip()\n",
    "    ratings = tv_series_item.find('span', {'class': 'ipl-rating-star__rating'}).text.strip()\n",
    "    votes = tv_series_item.find('span', {'name': 'nv'}).text.strip()\n",
    "\n",
    "    print(\"A) Name:\", name)\n",
    "    print(\"B) Year span:\", year_span)\n",
    "    print(\"C) Genre:\", genre)\n",
    "    print(\"D) Run time:\", run_time)\n",
    "    print(\"E) Ratings:\", ratings)\n",
    "    print(\"F) Votes:\", votes)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "datasets_link = soup.find('a', {'href': '/ml/datasets.php'})\n",
    "print(datasets_link)\n",
    "datasets_url = \"https://archive.ics.uci.edu\" + datasets_link['href']\n",
    "\n",
    "datasets_response = requests.get(datasets_url)\n",
    "datasets_soup = BeautifulSoup(datasets_response.text, 'html.parser')\n",
    "\n",
    "\n",
    "table = datasets_soup.find('table', {'cellpadding': '3'})\n",
    "\n",
    "rows = table.find_all('tr')[1:]\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all('td')\n",
    "    dataset_name = columns[0].text.strip()\n",
    "    data_type = columns[1].text.strip()\n",
    "    task = columns[2].text.strip()\n",
    "    attribute_type = columns[3].text.strip()\n",
    "    no_of_instances = columns[4].text.strip()\n",
    "    no_of_attributes = columns[5].text.strip()\n",
    "    year = columns[6].text.strip()\n",
    "\n",
    "    print(\"A) Dataset name:\", dataset_name)\n",
    "    print(\"B) Data type:\", data_type)\n",
    "    print(\"C) Task:\", task)\n",
    "    print(\"D) Attribute type:\", attribute_type)\n",
    "    print(\"E) No of instances:\", no_of_instances)\n",
    "    print(\"F) No of attributes:\", no_of_attributes)\n",
    "    print(\"G) Year:\", year)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52791a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
