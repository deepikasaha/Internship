{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed9425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facaf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "befe322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_shine_jobs(job_title, location, num_jobs=10):\n",
    "    # Step 1: Get the webpage\n",
    "    base_url = \"https://www.shine.com/\"\n",
    "    search_url = f\"{base_url}job-search/{job_title.replace(' ', '-').lower()}-jobs-in-{location.lower()}\"\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    job_listings = soup.find_all('div', class_='search_listing')\n",
    "    scraped_data = []\n",
    "\n",
    "    for job_listing in job_listings[:num_jobs]:\n",
    "        job_title = job_listing.find('div', class_='job_title').text.strip()\n",
    "        job_location = job_listing.find('span', class_='loc').text.strip()\n",
    "        company_name = job_listing.find('div', class_='comp_name').text.strip()\n",
    "        experience_required = job_listing.find('span', class_='exp').text.strip()\n",
    "\n",
    "        job_data = {\n",
    "            'Job Title': job_title,\n",
    "            'Job Location': job_location,\n",
    "            'Company Name': company_name,\n",
    "            'Experience Required': experience_required\n",
    "        }\n",
    "\n",
    "        scraped_data.append(job_data)\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    return df\n",
    "job_title = \"Data Analyst\"\n",
    "location = \"Bangalore\"\n",
    "num_jobs_to_scrape = 10\n",
    "\n",
    "jobs_dataframe = scrape_shine_jobs(job_title, location, num_jobs_to_scrape)\n",
    "\n",
    "print(jobs_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e9a11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21368321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_shine_jobs(job_title, location, num_jobs=10):\n",
    "    # Step 1: Get the webpage\n",
    "    base_url = \"https://www.shine.com/\"\n",
    "    search_url = f\"{base_url}job-search/{job_title.replace(' ', '-').lower()}-jobs-in-{location.lower()}\"\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    job_listings = soup.find_all('div', class_='search_listing')\n",
    "    scraped_data = []\n",
    "\n",
    "    for job_listing in job_listings[:num_jobs]:\n",
    "        job_title = job_listing.find('div', class_='job_title').text.strip()\n",
    "        job_location = job_listing.find('span', class_='loc').text.strip()\n",
    "        company_name = job_listing.find('div', class_='comp_name').text.strip()\n",
    "\n",
    "        job_data = {\n",
    "            'Job Title': job_title,\n",
    "            'Job Location': job_location,\n",
    "            'Company Name': company_name\n",
    "        }\n",
    "\n",
    "        scraped_data.append(job_data)\n",
    "\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "job_title = \"Data Scientist\"\n",
    "location = \"Bangalore\"\n",
    "num_jobs_to_scrape = 10\n",
    "\n",
    "jobs_dataframe = scrape_shine_jobs(job_title, location, num_jobs_to_scrape)\n",
    "\n",
    "print(jobs_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0caeb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59717fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?\n",
      "<>:13: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?\n",
      "/var/folders/lv/q9h0_4j97s56lbd8198k_tfw0000gn/T/ipykernel_894/1447519784.py:13: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?\n",
      "  return None()\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_shine_jobs(job_title, location_filter, salary_filter, num_jobs=10):\n",
    "    # Step 1: Get the webpage\n",
    "    base_url = \"https://www.shine.com/\"\n",
    "    search_url = f\"{base_url}job-search/{job_title.replace(' ', '-').lower()}-jobs\"\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    location_checkbox = soup.find('input', {'value': location_filter})\n",
    "    if location_checkbox:\n",
    "        location_checkbox['checked'] = True\n",
    "\n",
    "    salary_checkbox = soup.find('input', {'value': salary_filter})\n",
    "    if salary_checkbox:\n",
    "        salary_checkbox['checked'] = True\n",
    "\n",
    "    # Step 5: Scrape data for the first 10 jobs after applying filters\n",
    "    job_listings = soup.find_all('div', class_='search_listing')\n",
    "    scraped_data = []\n",
    "\n",
    "    for job_listing in job_listings[:num_jobs]:\n",
    "        job_title = job_listing.find('div', class_='job_title').text.strip()\n",
    "        job_location = job_listing.find('span', class_='loc').text.strip()\n",
    "        company_name = job_listing.find('div', class_='comp_name').text.strip()\n",
    "        experience_required = job_listing.find('span', class_='exp').text.strip()\n",
    "\n",
    "        job_data = {\n",
    "            'Job Title': job_title,\n",
    "            'Job Location': job_location,\n",
    "            'Company Name': company_name,\n",
    "            'Experience Required': experience_required\n",
    "        }\n",
    "\n",
    "        scraped_data.append(job_data)\n",
    "\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    return df\n",
    "job_title = \"Data Scientist\"\n",
    "location_filter = \"Delhi/NCR\"\n",
    "salary_filter = \"3-6 Lakhs\"\n",
    "num_jobs_to_scrape = 10\n",
    "\n",
    "jobs_dataframe = scrape_shine_jobs(job_title, location_filter, salary_filter, num_jobs_to_scrape)\n",
    "\n",
    "\n",
    "print(jobs_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ed4bb",
   "metadata": {},
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ecb238",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 7) (4209534542.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    search_query = \"sunglasses\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 7)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_sunglasses(num_listings=100):\n",
    "    base_url = \"https://www.flipkart.com/\"\n",
    "    search_query = \"sunglasses\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    search_url = f\"{base_url}search?q={search_query}\"\n",
    "    response = requests.get(search_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the search results. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    sunglasses_listings = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "    scraped_data = []\n",
    "    for listing in sunglasses_listings[:num_listings]:\n",
    "        brand = listing.find('div', {'class': '_2WkVRV'}).text.strip()\n",
    "        description = listing.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "        price = listing.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "\n",
    "        sunglasses_data = {\n",
    "            'Brand': brand,\n",
    "            'Product Description': description,\n",
    "            'Price': price\n",
    "        }\n",
    "\n",
    "        scraped_data.append(sunglasses_data)\n",
    "\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "sunglasses_dataframe = scrape_flipkart_sunglasses()\n",
    "\n",
    "print(sunglasses_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f21d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89cea838",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (1605038011.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 39\u001b[0;36m\u001b[0m\n\u001b[0;31m    iphone_reviews_dataframe = scrape_flipkart_reviews(flipkart_url, num_reviews=100\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_reviews(url, num_reviews=100):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    reviews = soup.find_all('div', {'class': '_27M-vq'})\n",
    "\n",
    "    scraped_data = []\n",
    "    for review in reviews[:num_reviews]:\n",
    "        rating = review.find('div', {'class': '_3LWZlK'}).text.strip()\n",
    "        summary = review.find('p', {'class': '_2-N8zT'}).text.strip()\n",
    "        full_review = review.find('div', {'class': 't-ZTKy'}).text.strip()\n",
    "\n",
    "        review_data = {\n",
    "            'Rating': rating,\n",
    "            'Review Summary': summary,\n",
    "            'Full Review': full_review\n",
    "        }\n",
    "\n",
    "        scraped_data.append(review_data)\n",
    "\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "    return df\n",
    "\n",
    "flipkart_url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "iphone_reviews_dataframe = scrape_flipkart_reviews(flipkart_url, num_reviews=100\n",
    "print(iphone_reviews_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc2f33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f598e42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scraped_data)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 42\u001b[0m sneakers_dataframe \u001b[38;5;241m=\u001b[39m scrape_flipkart_sneakers()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(sneakers_dataframe)\n",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mscrape_flipkart_sneakers\u001b[0;34m(num_sneakers)\u001b[0m\n\u001b[1;32m     25\u001b[0m scraped_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m listing \u001b[38;5;129;01min\u001b[39;00m sneakers_listings[:num_sneakers]:\n\u001b[0;32m---> 27\u001b[0m     brand \u001b[38;5;241m=\u001b[39m listing\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2WkVRV\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m     description \u001b[38;5;241m=\u001b[39m listing\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIRpwTa\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     29\u001b[0m     price \u001b[38;5;241m=\u001b[39m listing\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_30jeq3\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_sneakers(num_sneakers=100):\n",
    "    base_url = \"https://www.flipkart.com/\"\n",
    "    search_query = \"sneakers\"\n",
    "\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    search_url = f\"{base_url}search?q={search_query}\"\n",
    "    response = requests.get(search_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the search results. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    sneakers_listings = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "    \n",
    "    scraped_data = []\n",
    "    for listing in sneakers_listings[:num_sneakers]:\n",
    "        brand = listing.find('div', {'class': '_2WkVRV'}).text.strip()\n",
    "        description = listing.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "        price = listing.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "\n",
    "        sneakers_data = {\n",
    "            'Brand': brand,\n",
    "            'Product Description': description,\n",
    "            'Price': price\n",
    "        }\n",
    "\n",
    "        scraped_data.append(sneakers_data)\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "sneakers_dataframe = scrape_flipkart_sneakers()\n",
    "\n",
    "print(sneakers_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91586c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e02bd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scraped_data)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 44\u001b[0m top_quotes_dataframe \u001b[38;5;241m=\u001b[39m scrape_azquotes_top_quotes(num_quotes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(top_quotes_dataframe)\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mscrape_azquotes_top_quotes\u001b[0;34m(num_quotes)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     15\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m top_quotes_url \u001b[38;5;241m=\u001b[39m base_url \u001b[38;5;241m+\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/top-quotes\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(top_quotes_url)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_azquotes_top_quotes(num_quotes=1000):\n",
    "    base_url = \"https://www.azquotes.com/\"\n",
    "\n",
    "    \n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    top_quotes_url = base_url + soup.find('a', {'href': '/top-quotes'}).get('href')\n",
    "    response = requests.get(top_quotes_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the Top Quotes page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    quotes_list = soup.find_all('div', {'class': 'wrap-block'})\n",
    "    scraped_data = []\n",
    "\n",
    "    for quote in quotes_list[:num_quotes]:\n",
    "        quote_text = quote.find('a', {'class': 'title'}).text.strip()\n",
    "        author = quote.find('span', {'class': 'author'}).text.strip()\n",
    "        quote_type = quote.find('div', {'class': 'kw-box'}).text.strip()\n",
    "\n",
    "        quote_data = {\n",
    "            'Quote': quote_text,\n",
    "            'Author': author,\n",
    "            'Type Of Quote': quote_type\n",
    "        }\n",
    "\n",
    "        scraped_data.append(quote_data)\n",
    "\n",
    "    # Step 6: Create a DataFrame\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    return df\n",
    "top_quotes_dataframe = scrape_azquotes_top_quotes(num_quotes=1000)\n",
    "\n",
    "print(top_quotes_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b33d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca0bfa9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (27467451.py, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 52\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_prime_ministers_data():\n",
    "    base_url = \"https://www.jagranjosh.com/\"\n",
    "\n",
    "\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    gk_url = base_url + soup.find('a', {'href': '/general-knowledge'}).get('href')\n",
    "    response = requests.get(gk_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the GK page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "    prime_ministers_url = gk_url + soup.find('a', {'href': '/list-of-all-prime-ministers-of-india-1568288285-1'}).get('href')\n",
    "    response = requests.get(prime_ministers_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the Prime Ministers page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    prime_ministers_list = soup.find_all('tr', {'class': 'row2'})\n",
    "    scraped_data = []\n",
    "\n",
    "    for prime_minister in prime_ministers_list:\n",
    "        columns = prime_minister.find_all('td')\n",
    "\n",
    "        name = columns[0].text.strip()\n",
    "        born_dead = columns[1].text.strip()\n",
    "        term_of_office = columns[2].text.strip()\n",
    "        remarks = columns[3].text.strip()\n",
    "\n",
    "        prime_minister_data = {\n",
    "            'Name': name,\n",
    "            'Born-Dead': born_dead,\n",
    "            'Term of Office': term_of_office,\n",
    "            'Remarks': remarks\n",
    "        }\n",
    "\n",
    "        scraped_data.append(prime_minister_data)\n",
    "\n",
    "\n",
    "    df = pd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afaa9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9708fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link not found.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_expensive_cars():\n",
    "    base_url = \"https://www.motor1.com/\"\n",
    "\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    search_url = f\"{base_url}news/?q=50+most+expensive+cars\"\n",
    "    response = requests.get(search_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the search results. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    link = soup.find('a', {'href': '/news/377754/top-50-most-expensive-cars/'})\n",
    "    if not link:\n",
    "        print(\"Link not found.\")\n",
    "        return None\n",
    "\n",
    "    most_expensive_cars_url = base_url + link['href']\n",
    "    response = requests.get(most_expensive_cars_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page with the most expensive cars. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    cars_list = soup.find_all('div', {'class': 'slideshow-item'})\n",
    "    scraped_data = []\n",
    "\n",
    "    for car in cars_list[:50]:\n",
    "        car_name = car.find('h3').text.strip()\n",
    "        car_price = car.find('span', {'class': 'price'}).text.strip()\n",
    "\n",
    "        car_data = {\n",
    "            'Car Name': car_name,\n",
    "            'Price': car_price\n",
    "        }\n",
    "\n",
    "        scraped_data.append(car_data)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "most_expensive_cars_dataframe = scrape_most_expensive_cars()\n",
    "\n",
    "print(most_expensive_cars_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234ebc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
