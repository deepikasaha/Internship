{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27fe134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./Documents/anaconda3/lib/python3.11/site-packages (2.29.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./Documents/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in ./Documents/anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./Documents/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./Documents/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./Documents/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./Documents/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in ./Documents/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d331349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Header                           Text\n",
      "0      h1                      Main Page\n",
      "1      h1            Welcome toWikipedia\n",
      "2      h2  From today's featured article\n",
      "3      h2               Did you knowÂ ...\n",
      "4      h2                    In the news\n",
      "5      h2                    On this day\n",
      "6      h2     From today's featured list\n",
      "7      h2       Today's featured picture\n",
      "8      h2       Other areas of Wikipedia\n",
      "9      h2    Wikipedia's sister projects\n",
      "10     h2            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_wikipedia_headers(url):\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    data = {'Header': [], 'Text': []}\n",
    "    for header in headers:\n",
    "        data['Header'].append(header.name)\n",
    "        data['Text'].append(header.get_text(strip=True))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wikipedia_url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "    headers_df = get_wikipedia_headers(wikipedia_url)\n",
    "\n",
    "    print(headers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d50952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862c8c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./Documents/anaconda3/lib/python3.11/site-packages (2.29.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./Documents/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in ./Documents/anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./Documents/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./Documents/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./Documents/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./Documents/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in ./Documents/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24332093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_president_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    president_data = []\n",
    "    \n",
    "    for row in soup.find_all('tr')[1:]:  \n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].text.strip()\n",
    "        term_of_office = columns[1].text.strip()\n",
    "        president_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "    \n",
    "    return president_data\n",
    "\n",
    "def main():\n",
    "    url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "    president_data = get_president_data(url)\n",
    "    \n",
    "    df = pd.DataFrame(president_data)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b24caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6339592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "               Team Matches Points Rating\n",
      "0        India\\nIND      55  6,640    121\n",
      "1    Australia\\nAUS      42  4,926    117\n",
      "2  South Africa\\nSA      34  3,750    110\n",
      "3     Pakistan\\nPAK      36  3,922    109\n",
      "4   New Zealand\\nNZ      43  4,399    102\n",
      "5      England\\nENG      38  3,777     99\n",
      "6     Sri Lanka\\nSL      47  4,134     88\n",
      "7   Bangladesh\\nBAN      44  3,836     87\n",
      "8  Afghanistan\\nAFG      30  2,533     84\n",
      "9   West Indies\\nWI      38  2,582     68\n",
      "\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_odi_team_rankings():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    teams_data = []\n",
    "\n",
    "    for row in soup.select('.table tbody tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        team = columns[1].text.strip()\n",
    "        matches = columns[2].text.strip()\n",
    "        points = columns[3].text.strip()\n",
    "        rating = columns[4].text.strip()\n",
    "        teams_data.append({'Team': team, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return teams_data\n",
    "\n",
    "def get_odi_batsmen_rankings():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    batsmen_data = []\n",
    "\n",
    "    for row in soup.select('.table tbody tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        player = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        batsmen_data.append({'Player': player, 'Team': team, 'Rating': rating})\n",
    "\n",
    "    return batsmen_data\n",
    "\n",
    "def get_odi_bowlers_rankings():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    bowlers_data = []\n",
    "\n",
    "    for row in soup.select('.table tbody tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        player = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        bowlers_data.append({'Player': player, 'Team': team, 'Rating': rating})\n",
    "\n",
    "    return bowlers_data\n",
    "\n",
    "def main():\n",
    "    teams_data = get_odi_team_rankings()\n",
    "    df_teams = pd.DataFrame(teams_data)\n",
    "    print(\"Top 10 ODI Teams:\")\n",
    "    print(df_teams)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Part b) Top 10 ODI Batsmen\n",
    "    batsmen_data = get_odi_batsmen_rankings()\n",
    "    df_batsmen = pd.DataFrame(batsmen_data)\n",
    "    print(\"Top 10 ODI Batsmen:\")\n",
    "    print(df_batsmen)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Part c) Top 10 ODI Bowlers\n",
    "    bowlers_data = get_odi_bowlers_rankings()\n",
    "    df_bowlers = pd.DataFrame(bowlers_data)\n",
    "    print(\"Top 10 ODI Bowlers:\")\n",
    "    print(df_bowlers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f30c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515ec1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams:\n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n",
      "\n",
      "\n",
      "Top 10 Women's ODI Batting Players:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "\n",
      "Top 10 Women's ODI All-rounders:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_womens_odi_team_rankings():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    teams_data = []\n",
    "\n",
    "    # Extracting data for the top 10 Women's ODI teams\n",
    "    for row in soup.select('.table tbody tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        team = columns[1].text.strip()\n",
    "        matches = columns[2].text.strip()\n",
    "        points = columns[3].text.strip()\n",
    "        rating = columns[4].text.strip()\n",
    "        teams_data.append({'Team': team, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return teams_data\n",
    "\n",
    "def get_womens_odi_batting_players_rankings():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    batting_players_data = []\n",
    "\n",
    "    for row in soup.select('.table tbody tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        player = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        batting_players_data.append({'Player': player, 'Team': team, 'Rating': rating})\n",
    "\n",
    "    return batting_players_data\n",
    "\n",
    "def get_womens_odi_all_rounders_rankings():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    all_rounders_data = []\n",
    "\n",
    "    for row in soup.select('.table tbody tr')[:10]:\n",
    "        columns = row.find_all('td')\n",
    "        player = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        all_rounders_data.append({'Player': player, 'Team': team, 'Rating': rating})\n",
    "\n",
    "    return all_rounders_data\n",
    "\n",
    "def main():\n",
    "    teams_data = get_womens_odi_team_rankings()\n",
    "    df_teams = pd.DataFrame(teams_data)\n",
    "    print(\"Top 10 Women's ODI Teams:\")\n",
    "    print(df_teams)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    batting_players_data = get_womens_odi_batting_players_rankings()\n",
    "    df_batting_players = pd.DataFrame(batting_players_data)\n",
    "    print(\"Top 10 Women's ODI Batting Players:\")\n",
    "    print(df_batting_players)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    all_rounders_data = get_womens_odi_all_rounders_rankings()\n",
    "    df_all_rounders = pd.DataFrame(all_rounders_data)\n",
    "    print(\"Top 10 Women's ODI All-rounders:\")\n",
    "    print(df_all_rounders)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe39c4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Scrape the data\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m df_news \u001b[38;5;241m=\u001b[39m scrape_cnbc_news(url)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_news \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_news)\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mscrape_cnbc_news\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Extract data from each article\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[0;32m---> 24\u001b[0m     headline \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCard-titleLink\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     25\u001b[0m     time \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     26\u001b[0m     link \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCard-titleLink\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "        headlines = []\n",
    "        times = []\n",
    "        news_links = []\n",
    "        \n",
    "        for article in articles:\n",
    "            headline = article.find('a', class_='Card-titleLink').text.strip()\n",
    "            time = article.find('time').text.strip()\n",
    "            link = article.find('a', class_='Card-titleLink')['href']\n",
    "            \n",
    "            headlines.append(headline)\n",
    "            times.append(time)\n",
    "            news_links.append(link)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Headline': headlines,\n",
    "            'Time': times,\n",
    "            'News Link': news_links\n",
    "        })\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to fetch content. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.cnbc.com/world/?region=world\"\n",
    "    \n",
    "    df_news = scrape_cnbc_news(url)\n",
    "    \n",
    "    if df_news is not None:\n",
    "        print(df_news)\n",
    "        \n",
    "        df_news.to_csv('cnbc_news.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb282a50",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     21\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m     news_data \u001b[38;5;241m=\u001b[39m scrape_cnbc_news(url)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from the collected data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(news_data)\n",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m, in \u001b[0;36mscrape_cnbc_news\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCard-titleContainer\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     12\u001b[0m     headline \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 13\u001b[0m     time \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     14\u001b[0m     news_link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     16\u001b[0m     news_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeadline\u001b[39m\u001b[38;5;124m'\u001b[39m: headline, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m: time, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNews Link\u001b[39m\u001b[38;5;124m'\u001b[39m: news_link})\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    news_data = []\n",
    "\n",
    "    for article in soup.find_all('div', class_='Card-titleContainer'):\n",
    "        headline = article.find('a').text.strip()\n",
    "        time = article.find('time').text.strip()\n",
    "        news_link = 'https://www.cnbc.com' + article.find('a')['href'].strip()\n",
    "\n",
    "        news_data.append({'Headline': headline, 'Time': time, 'News Link': news_link})\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.cnbc.com/world/?region=world'\n",
    "    news_data = scrape_cnbc_news(url)\n",
    "\n",
    "    df = pd.DataFrame(news_data)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd44d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "316f7704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles_data = []\n",
    "\n",
    "    for article in soup.find_all('div', class_='pod-listing')[:10]:  \n",
    "        title = article.find('a', class_='pod-listing-header').text.strip()\n",
    "        authors = article.find('div', class_='text-s').text.strip()\n",
    "        date = article.find('div', class_='text-m').text.strip()\n",
    "        paper_url = 'https://www.journals.elsevier.com' + article.find('a', class_='pod-listing-header')['href'].strip()\n",
    "\n",
    "        articles_data.append({'Paper Title': title, 'Authors': authors, 'Published Date': date, 'Paper URL': paper_url})\n",
    "\n",
    "    return articles_data\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "    articles_data = scrape_most_downloaded_articles(url)\n",
    "\n",
    "    df = pd.DataFrame(articles_data)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74c67b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "142246c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_details(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    restaurants_data = []\n",
    "\n",
    "    for card in soup.find_all('div', class_='restnt-card-container'):\n",
    "        name = card.find('div', class_='restnt-info cursor').find('a').text.strip()\n",
    "        cuisine = card.find('div', class_='restnt-cuisine').text.strip()\n",
    "        location = card.find('div', class_='restnt-loc').text.strip()\n",
    "        ratings = card.find('div', class_='restnt-rating').text.strip()\n",
    "        image_url = card.find('div', class_='restnt-photo')['style'].split(\"('\")[1].split(\"')\")[0].strip()\n",
    "\n",
    "        restaurants_data.append({'Restaurant Name': name, 'Cuisine': cuisine, 'Location': location, 'Ratings': ratings, 'Image URL': image_url})\n",
    "\n",
    "    return restaurants_data\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "    restaurants_data = scrape_dineout_details(url)\n",
    "\n",
    "    df = pd.DataFrame(restaurants_data)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99d46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
