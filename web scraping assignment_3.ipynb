{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    product = input(\"Enter the product you want to search: \")\n",
    "    url = \"https://www.amazon.in/s?k=\" + product\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = soup.find_all('div', class_='s-result-item')\n",
    "    for product in products:\n",
    "        title = product.find('a', class_='a-link-normal s-link-style a-text-normal').text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests\n",
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "    encoded_product_name = product_name.replace(' ', '+')\n",
    "\n",
    "    url = f'https://www.amazon.in/s?k={encoded_product_name}'\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "    \n",
    "        if response.status_code == 200:\n",
    "        \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "            product_list = soup.find_all('div', {'data-asin': True})\n",
    "\n",
    "        \n",
    "            print(f\"Search Results for '{product_name}':\\n\")\n",
    "            for product in product_list:\n",
    "                product_title = product.find('span', {'class': 'a-text-normal'}).text.strip()\n",
    "                product_price = product.find('span', {'class': 'a-offscreen'})\n",
    "\n",
    "                if product_price:\n",
    "                    product_price = product_price.text.strip()\n",
    "                else:\n",
    "                    product_price = 'Price not available'\n",
    "\n",
    "                print(f\"Title: {product_title}\\nPrice: {product_price}\\n{'-'*50}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    user_input = input(\"Enter the product you want to search on Amazon: \")\n",
    "    \n",
    "\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e099c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(product):\n",
    "    product_url = \"https://www.amazon.in\" + product.find('a', {'class': 'a-link-normal'})['href']\n",
    "    product_response = requests.get(product_url)\n",
    "    product_soup = BeautifulSoup(product_response.content, 'html.parser')\n",
    "\n",
    "    brand_name = product_soup.find('span', {'class': 'a-size-base-plus a-color-base'}).text.strip()\n",
    "\n",
    "    product_title = product_soup.find('span', {'id': 'productTitle'})\n",
    "    name_of_product = product_title.text.strip() if product_title else '-'\n",
    "\n",
    "    price = product_soup.find('span', {'id': 'priceblock_ourprice'})\n",
    "    price = price.text.strip() if price else '-'\n",
    "\n",
    "    return_exchange = product_soup.find('div', {'id': 'RETURNS_POLICY'})\n",
    "    return_exchange = return_exchange.text.strip() if return_exchange else '-'\n",
    "\n",
    "    expected_delivery = product_soup.find('div', {'id': 'ddmDeliveryMessage'})\n",
    "    expected_delivery = expected_delivery.text.strip() if expected_delivery else '-'\n",
    "\n",
    "    availability = product_soup.find('div', {'id': 'availability'})\n",
    "    availability = availability.text.strip() if availability else '-'\n",
    "\n",
    "    return {\n",
    "        'Brand Name': brand_name,\n",
    "        'Name of the Product': name_of_product,\n",
    "        'Price': price,\n",
    "        'Return/Exchange': return_exchange,\n",
    "        'Expected Delivery': expected_delivery,\n",
    "        'Availability': availability,\n",
    "        'Product URL': product_url\n",
    "    }\n",
    "\n",
    "def search_amazon(product_name, num_pages=3):    # ... (Same code as before)\n",
    "\n",
    "    columns = ['Brand Name', 'Name of the Product', 'Price', 'Return/Exchange', 'Expected Delivery', 'Availability', 'Product URL']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for page in range(1, num_pages + 1):\n",
    "        page_url = f'https://www.amazon.in/s?k={encoded_product_name}&page={page}'\n",
    "        page_response = requests.get(page_url, headers=headers)\n",
    "\n",
    "        if page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parse\n",
    "            product_list = page_soup.find_all('div', {'data-asin': True})\n",
    "\n",
    "        \n",
    "            for product in product_list:\n",
    "                details = scrape_product_details(product)\n",
    "                df = df.append(details, ignore_index=True)\n",
    "    df.to_csv('amazon_search_results.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b705c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ce41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(\"https://images.google.com\")\n",
    "    search_bar = driver.find_element(\"name\", \"q\")\n",
    "    search_button = driver.find_element(\"xpath\", \"//button[@type='submit']\")\n",
    "\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_button.click()\n",
    "\n",
    "    for _ in range(3):\n",
    "        driver.find_element(\"xpath\", \"//body\").send_keys(Keys.END)\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    \n",
    "    image_elements = soup.find_all(\"img\", {\"class\": \"rg_i\"})\n",
    "    image_urls = [element[\"src\"] for element in image_elements if \"src\" in element.attrs]\n",
    "\n",
    "    \n",
    "    image_urls = image_urls[:num_images]\n",
    "\n",
    "    \n",
    "    save_dir = f\"{keyword}_images\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    for i, image_url in enumerate(image_urls):\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        extension = os.path.splitext(urlparse(image_url).path)[-1]\n",
    "\n",
    "        with open(os.path.join(save_dir, f\"{keyword}_{i+1}{extension}\"), \"wb\") as img_file:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                img_file.write(chunk)\n",
    "\n",
    "    print(f\"{num_images} {keyword} images scraped and saved in '{save_dir}'.\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "    for keyword in keywords:\n",
    "        scrape_images(keyword, num_images=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphone_details(product):\n",
    "    brand_name = product.find('div', {'class': '_4rR01T'}).text.strip()\n",
    "    smartphone_name = product.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "\n",
    "    product_url = 'https://www.flipkart.com' + product.find('a', {'class': 'IRpwTa'})['href']\n",
    "    product_response = requests.get(product_url)\n",
    "    product_soup = BeautifulSoup(product_response.content, 'html.parser')\n",
    "\n",
    "    color = product_soup.find('div', {'class': '_3moNKs'}).text.strip()\n",
    "\n",
    "    specifications = product_soup.find_all('div', {'class': '_1AtVbE'})\n",
    "    specifications_dict = {spec.find('div', {'class': '_2Kp3n6'}).text.strip(): spec.find('li', {'class': 'rgWa7D'}).text.strip() for spec in specifications}\n",
    "\n",
    "    ram = specifications_dict.get('RAM', '-')\n",
    "    storage = specifications_dict.get('Internal Storage', '-')\n",
    "    primary_camera = specifications_dict.get('Primary Camera', '-')\n",
    "    secondary_camera = specifications_dict.get('Secondary Camera', '-')\n",
    "    display_size = specifications_dict.get('Display Size', '-')\n",
    "    battery_capacity = specifications_dict.get('Battery Capacity', '-')\n",
    "    \n",
    "    price = product.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "\n",
    "    return {\n",
    "        'Brand Name': brand_name,\n",
    "        'Smartphone Name': smartphone_name,\n",
    "        'Colour': color,\n",
    "        'RAM': ram,\n",
    "        'Storage(ROM)': storage,\n",
    "        'Primary Camera': primary_camera,\n",
    "        'Secondary Camera': secondary_camera,\n",
    "        'Display Size': display_size,\n",
    "        'Battery Capacity': battery_capacity,\n",
    "        'Price': price,\n",
    "        'Product URL': product_url\n",
    "    }\n",
    "\n",
    "def search_flipkart_smartphones(search_query, num_results=10):\n",
    "    url = f'https://www.flipkart.com/search?q={search_query}&page=1'\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        product_list = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "\n",
    "        columns = ['Brand Name', 'Smartphone Name', 'Colour', 'RAM', 'Storage(ROM)', 'Primary Camera',\n",
    "                   'Secondary Camera', 'Display Size', 'Battery Capacity', 'Price', 'Product URL']\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        \n",
    "        for product in product_list[:num_results]:\n",
    "            details = scrape_smartphone_details(product)\n",
    "\n",
    "        \n",
    "            df = df.append(details, ignore_index=True)\n",
    "\n",
    "    \n",
    "        df.to_csv(f'{search_query}_search_results.csv', index=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    user_input = input(\"Enter the smartphone you want to search on Flipkart: \")\n",
    "\n",
    "\n",
    "    search_flipkart_smartphones(user_input, num_results=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_coordinates(api_key, city):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\n",
    "        'address': city,\n",
    "        'key': api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if data['status'] == 'OK' and data.get('results'):\n",
    "            location = data['results'][0]['geometry']['location']\n",
    "            latitude = location['lat']\n",
    "            longitude = location['lng']\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            print(\"Error: Unable to fetch coordinates.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "\n",
    "\n",
    "    city_input = input(\"Enter the city to get geospatial coordinates: \")\n",
    "\n",
    "    coordinates = get_coordinates(api_key, city_input)\n",
    "\n",
    "    if coordinates:\n",
    "        print(f\"Geospatial Coordinates for {city_input}: Latitude {coordinates[0]}, Longitude {coordinates[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602599aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract details from the website based on its structure\n",
    "        laptop_listings = soup.find_all('div', class_='Top2020-srchresult')\n",
    "\n",
    "        details_list = []\n",
    "        for laptop in laptop_listings:\n",
    "            laptop_name = laptop.find('div', class_='TopNumbeHeading').text.strip()\n",
    "            specifications = laptop.find_all('div', class_='Spcs-details')\n",
    "            \n",
    "            specs_dict = {}\n",
    "            for spec in specifications:\n",
    "                key = spec.find('div', class_='Spcs-details-left').text.strip()\n",
    "                value = spec.find('div', class_='Spcs-details-right').text.strip()\n",
    "                specs_dict[key] = value\n",
    "\n",
    "            details = {\n",
    "                'Laptop Name': laptop_name,\n",
    "                **specs_dict\n",
    "            }\n",
    "\n",
    "            details_list.append(details)\n",
    "\n",
    "        return details_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    digit_url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "\n",
    "\n",
    "    laptop_details = scrape_gaming_laptops(digit_url)\n",
    "\n",
    "    if laptop_details:\n",
    "\n",
    "        df = pd.DataFrame(laptop_details)\n",
    "        df.to_csv('gaming_laptops_details.csv', index=False)\n",
    "        print(\"Details of best gaming laptops saved to 'gaming_laptops_details.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ca588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "        billionaire_listings = soup.find_all('div', class_='personRow')\n",
    "\n",
    "        details_list = []\n",
    "        for billionaire in billionaire_listings:\n",
    "            rank = billionaire.find('div', class_='rank').text.strip()\n",
    "            name = billionaire.find('div', class_='name').text.strip()\n",
    "            net_worth = billionaire.find('div', class_='netWorth').text.strip()\n",
    "            age = billionaire.find('div', class_='age').text.strip()\n",
    "            citizenship = billionaire.find('div', class_='countryOfCitizenship').text.strip()\n",
    "            source = billionaire.find('div', class_='source').text.strip()\n",
    "            industry = billionaire.find('div', class_='category').text.strip()\n",
    "\n",
    "            details = {\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net Worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            }\n",
    "\n",
    "            details_list.append(details)\n",
    "\n",
    "        return details_list\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    forbes_url = 'https://www.forbes.com/billionaires/'\n",
    "\n",
    "\n",
    "    billionaires_details = scrape_forbes_billionaires(forbes_url)\n",
    "\n",
    "    if billionaires_details:\n",
    "\n",
    "        df = pd.DataFrame(billionaires_details)\n",
    "        df.to_csv('forbes_billionaires_details.csv', index=False)\n",
    "        print(\"Details of billionaires saved to 'forbes_billionaires_details.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8de104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import json\n",
    "\n",
    "def get_youtube_comments(api_key, video_id, max_results=500):\n",
    "    youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    video_response = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "\n",
    "    video_title = video_response['items'][0]['snippet']['title']\n",
    "    print(f\"Video Title: {video_title}\")\n",
    "\n",
    "\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        comment_response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=min(100, max_results - len(comments)),\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in comment_response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_info = {\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Comment Upvotes': comment['likeCount'],\n",
    "                'Time Posted': comment['publishedAt']\n",
    "            }\n",
    "            comments.append(comment_info)\n",
    "\n",
    "        next_page_token = comment_response.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    \n",
    "    video_id = 'YOUR_VIDEO_ID'\n",
    "\n",
    "    video_comments = get_youtube_comments(api_key, video_id, max_results=500)\n",
    "\n",
    "    if video_comments:\n",
    "        # Print the extracted comments\n",
    "        for i, comment in enumerate(video_comments):\n",
    "            print(f\"\\nComment {i + 1}:\")\n",
    "            print(f\"Text: {comment['Comment']}\")\n",
    "            print(f\"Upvotes: {comment['Comment Upvotes']}\")\n",
    "            print(f\"Time Posted: {comment['Time Posted']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90450ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels_in_london(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "        hostel_listings = soup.find_all('div', class_='property-card')\n",
    "\n",
    "        details_list = []\n",
    "        for hostel in hostel_listings:\n",
    "            hostel_name = hostel.find('h2', class_='title').text.strip()\n",
    "            distance = hostel.find('span', class_='description').text.strip()\n",
    "            ratings = hostel.find('div', class_='score orange big').text.strip()\n",
    "            total_reviews = hostel.find('div', class_='reviews').text.strip().split()[0]\n",
    "            overall_reviews = hostel.find('div', class_='reviews').text.strip().split()[-1]\n",
    "            privates_price = hostel.find('div', class_='price-col').text.strip()\n",
    "            dorms_price = hostel.find('div', class_='price-col dorms').text.strip()\n",
    "\n",
    "    \n",
    "            facilities_list = [facility.text.strip() for facility in hostel.find_all('div', class_='facilities')]\n",
    "            facilities = ', '.join(facilities_list)\n",
    "\n",
    "\n",
    "            description = hostel.find('div', class_='more-info').text.strip()\n",
    "\n",
    "            details = {\n",
    "                'Hostel Name': hostel_name,\n",
    "                'Distance from City Centre': distance,\n",
    "                'Ratings': ratings,\n",
    "                'Total Reviews': total_reviews,\n",
    "                'Overall Reviews': overall_reviews,\n",
    "                'Privates from Price': privates_price,\n",
    "                'Dorms from Price': dorms_price,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_hostels_in_london(\"https://www.hostelworld.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ee9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
