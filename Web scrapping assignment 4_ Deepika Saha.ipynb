{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ba569f",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da4b2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f36af65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in ./Documents/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: requests in ./Documents/anaconda3/lib/python3.11/site-packages (2.29.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./Documents/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./Documents/anaconda3/lib/python3.11/site-packages (from requests) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7dd7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c354147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "page_content = response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f14344",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitable sortable jquery-tablesorter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m video_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:  \u001b[38;5;66;03m# Skip the header row\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     cells \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     rank \u001b[38;5;241m=\u001b[39m cells[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "table = soup.find(\"table\", class_=\"wikitable sortable jquery-tablesorter\")\n",
    "video_data = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    rank = cells[0].text.strip()\n",
    "    name = cells[1].text.strip()\n",
    "    artist = cells[2].text.strip()\n",
    "    upload_date = cells[3].text.strip()\n",
    "    views = cells[4].text.strip()\n",
    "    video_data.append({\n",
    "        \"Rank\": rank,\n",
    "        \"Name\": name,\n",
    "        \"Artist\": artist,\n",
    "        \"Upload date\": upload_date,\n",
    "        \"Views\": views\n",
    "    })\n",
    "for video in video_data:\n",
    "    print(video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61141439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5baf8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m fixtures_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/international/fixtures\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m----> 8\u001b[0m fixtures_url \u001b[38;5;241m=\u001b[39m url \u001b[38;5;241m+\u001b[39m fixtures_link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m fixtures_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(fixtures_url)\n\u001b[1;32m     10\u001b[0m fixtures_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(fixtures_response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "fixtures_link = soup.find('a', {'href': '/international/fixtures'})\n",
    "fixtures_url = url + fixtures_link['href']\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.text, 'html.parser')\n",
    "fixtures = fixtures_soup.find_all('div', {'class': 'fixture__format-info'})\n",
    "\n",
    "for fixture in fixtures:\n",
    "    series = fixture.find('span', {'class': 'u-unskewed-text'}).text.strip()\n",
    "    place = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "    date = fixture.find('div', {'class': 'fixture__full-date'}).text.strip()\n",
    "    time = fixture.find('span', {'class': 'fixture__time'}).text.strip()\n",
    "\n",
    "    print(\"A) Series:\", series)\n",
    "    print(\"B) Place:\", place)\n",
    "    print(\"C) Date:\", date)\n",
    "    print(\"D) Time:\", time)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d1366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd8640a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m indian_economy_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/economy/india-statistics.php\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m----> 8\u001b[0m indian_economy_url \u001b[38;5;241m=\u001b[39m url \u001b[38;5;241m+\u001b[39m indian_economy_link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m indian_economy_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(indian_economy_url)\n\u001b[1;32m     11\u001b[0m indian_economy_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(indian_economy_response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indian_economy_link = soup.find('a', {'href': '/economy/india-statistics.php'})\n",
    "indian_economy_url = url + indian_economy_link['href']\n",
    "\n",
    "indian_economy_response = requests.get(indian_economy_url)\n",
    "indian_economy_soup = BeautifulSoup(indian_economy_response.text, 'html.parser')\n",
    "\n",
    "\n",
    "state_gdp_link = indian_economy_soup.find('a', {'href': '/economy/indian-states-gdp.php'})\n",
    "state_gdp_url = url + state_gdp_link['href']\n",
    "\n",
    "state_gdp_response = requests.get(state_gdp_url)\n",
    "state_gdp_soup = BeautifulSoup(state_gdp_response.text, 'html.parser')\n",
    "\n",
    "table = state_gdp_soup.find('table', {'id': 'table_id'})\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows[1:]: \n",
    "    columns = row.find_all('td')\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gsdp_1819 = columns[2].text.strip()\n",
    "    gsdp_1920 = columns[3].text.strip()\n",
    "    share_1819 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "\n",
    "    print(\"A) Rank:\", rank)\n",
    "    print(\"B) State:\", state)\n",
    "    print(\"C) GSDP(18-19):\", gsdp_1819)\n",
    "    print(\"D) GSDP(19-20):\", gsdp_1920)\n",
    "    print(\"E) Share(18-19):\", share_1819)\n",
    "    print(\"F) GDP($ billion):\", gdp_billion)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cba45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea225e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m repo_cards \u001b[38;5;241m=\u001b[39m trending_repos_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBox-row\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo_card \u001b[38;5;129;01min\u001b[39;00m repo_cards:\n\u001b[0;32m---> 24\u001b[0m     repo_title \u001b[38;5;241m=\u001b[39m repo_card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     25\u001b[0m     repo_desc \u001b[38;5;241m=\u001b[39m repo_card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol-9\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     26\u001b[0m     contributors_count \u001b[38;5;241m=\u001b[39m repo_card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink--muted\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "trending_repos_link = soup.find('a', {'href': '/trending'})\n",
    "trending_repos_url = \"https://github.com\" + trending_repos_link['href']\n",
    "\n",
    "trending_repos_response = requests.get(trending_repos_url)\n",
    "trending_repos_soup = BeautifulSoup(trending_repos_response.text, 'html.parser')\n",
    "\n",
    "repo_cards = trending_repos_soup.find_all('article', {'class': 'Box-row'})\n",
    "\n",
    "for repo_card in repo_cards:\n",
    "    repo_title = repo_card.find('h1', {'class': 'h3'}).text.strip()\n",
    "    repo_desc = repo_card.find('p', {'class': 'col-9'}).text.strip()\n",
    "    contributors_count = repo_card.find('a', {'class': 'Link--muted'}).text.strip()\n",
    "    language_used = repo_card.find('span', {'itemprop': 'programmingLanguage'})\n",
    "    language_used = language_used.text.strip() if language_used else 'Not specified'\n",
    "\n",
    "    print(\"A) Repository title:\", repo_title)\n",
    "    print(\"B) Repository description:\", repo_desc)\n",
    "    print(\"C) Contributors count:\", contributors_count)\n",
    "    print(\"D) Language used:\", language_used)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97d5794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c8a15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "hot_100_link = soup.find('a', {'href': '/charts/hot-100/'})\n",
    "hot_100_url = \"https://www.billboard.com\" + hot_100_link['href']\n",
    "\n",
    "\n",
    "hot_100_response = requests.get(hot_100_url)\n",
    "hot_100_soup = BeautifulSoup(hot_100_response.text, 'html.parser')\n",
    "\n",
    "\n",
    "song_cards = hot_100_soup.find_all('li', {'class': 'chart-list__element'})\n",
    "\n",
    "for song_card in song_cards:\n",
    "    song_name = song_card.find('span', {'class': 'chart-element__information__song'}).text.strip()\n",
    "    artist_name = song_card.find('span', {'class': 'chart-element__information__artist'}).text.strip()\n",
    "    last_week_rank = song_card.find('span', {'class': 'chart-element__meta text--center color--secondary text--last'}).text.strip()\n",
    "    peak_rank = song_card.find('span', {'class': 'chart-element__meta text--center color--secondary text--peak'}).text.strip()\n",
    "    weeks_on_board = song_card.find('span', {'class': 'chart-element__meta text--center color--secondary text--week'}).text.strip()\n",
    "\n",
    "    print(\"A) Song name:\", song_name)\n",
    "    print(\"B) Artist name:\", artist_name)\n",
    "    print(\"C) Last week rank:\", last_week_rank)\n",
    "    print(\"D) Peak rank:\", peak_rank)\n",
    "    print(\"E) Weeks on board:\", weeks_on_board)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4762429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc3b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
